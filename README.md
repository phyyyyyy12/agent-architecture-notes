# agent-architecture-notes
深入解析 LLM 认知架构：从 CoT、ReAct、ToT 到 Reflexion 与 RL 的演进闭环
### 📝 今日“神经权重更新”总结报告
#### 1. 核心概念层级图 (俄罗斯套娃)

所有的概念其实是层层递进的，不是孤立的：

* **基座 (Base):** **LLM** (大模型本身)
* **第一层 (脑):** **CoT (思维链)**  让模型**把话说清楚**，显式写出逻辑。
* *JD 考点:* 会写 Prompt，懂 "Let's think step by step"。


* **第二层 (手):** **ReAct (推理+行动)**  CoT + **工具调用 (Tools)**。
* *JD 考点:* 会写代码循环，让 AI 联网、查库、调 API。


* **第三层 (网):** **ToT (思维树)**  多路 CoT + **评估与回溯 (Search)**。
* *JD 考点:* 懂算法 (DFS/BFS)，能解决复杂推理难题。


* **第四层 (镜):** **Reflexion (自我反思)**  失败后**看报错** + 存入**记忆 (RAG)**。
* *JD 考点:* 懂如何提升成功率，懂长短期记忆管理。


* **外挂 (锁):** **HITL (Human in the Loop)**  关键时刻**按暂停**，让人类确认。
* *JD 考点:* 懂安全，懂系统交互设计。



#### 2. 工程与学术的“翻译对照”

| 概念 | 本质是啥？ | 为什么 JD 要写？ | 实际工程里的样子 |
| --- | --- | --- | --- |
| **CoT** | **说话方式** | 它是最基本的 Prompt 技巧。 | `System: Please explain your reasoning...` |
| **ReAct** | **循环结构** | 代表你有开发 Agent 的能力。 | `while True: think() -> act() -> observe()` |
| **ToT** | **搜索算法** | 代表能处理高难度任务。 | 生成 A/B/C 三个方案，不要的剪掉，好的留下。 |
| **Reflexion** | **打补丁** | 代表能做自我纠错系统。 | 存一张“反思小纸条”到数据库，下次别犯错。 |
| **RL** | **系统升级** | 代表能训练模型。 | 把“反思后的正确数据”喂给模型，把补丁变成直觉。 |

#### 3. 终极心法：AI 进化的闭环

没错！这就是 **Reflexion（以及所有基于 In-Context Learning 的 Agent）** 面临的最大物理瓶颈：**Context Window（上下文窗口）有限**。

如果 Agent 试错 100 次，产生 100 张“反思小纸条”，把它们全塞进 Prompt 里，结果只有两个：

1. **挤爆了：** 超过了 token 上限（比如 8k 或 128k），模型报错。
2. **迷糊了：** “纸条”太多，模型注意力分散（Lost in the Middle），反而不知道该听哪一条。

所以，在工程实现上，我们绝对不会“傻傻地一直往后追加”。我们有一套**“记忆管理系统” (Memory Management System)** 来解决这个问题。

主要有三招：**遗忘（FIFO）、压缩（Summarization）和 归档（Vector DB）。**

### 第一招：只留最新的（Sliding Window / FIFO）

**策略：** 桌子上只能放 5 张纸条。

* 当有了第 6 条反思时，把**最旧的那张**扔进垃圾桶。
* **逻辑：** 我们假设最近的错误最具有参考价值，或者最早的错误已经不再犯了。
* **缺点：** 可能会“虽然学会了微积分，但忘记了加减法”，导致老毛病复发。

### 第二招：把纸条抄写成手册（Summarization）

**策略：** 当纸条攒够 10 张时，触发一个“整理任务”。

* 让 LLM 把这 10 条反思读一遍，然后**总结**成一条更精炼的“生存法则”。
* **例子：**
* *原纸条1：* 遇到火不要用木剑打。
* *原纸条2：* 遇到火不要用草系魔法。
* *原纸条3：* 遇到火穿防火衣。
* **压缩后：** **“对抗火属性敌人时，需装备防火防具并禁止使用易燃武器。”**

* **结果：** 1000 个 token 变成了 50 个 token，腾出了空间。

### 第三招：建一个图书馆（Vector Database / RAG）—— **这是目前的主流做法**

**策略：** 既然桌子（Prompt）放不下，我就买个大书架（数据库）。
我不把所有反思都塞进 Prompt，而是把它们存进 **向量数据库（如 Chroma, Pinecone）**。

* **存（Save）：**
每次反思生成后，存入数据库：`{ "情境": "打虎先锋", "反思": "不要贪刀，他的硬直时间很短" }`。
* **取（Retrieve）：**
当 Agent **下一次** 遇到“虎先锋”或者类似的 Boss 时，代码会先去数据库里搜：*“有没有关于打类似 Boss 的经验？”*
* 数据库返回 3 条相关的反思。
* 代码把这 **3 条相关的** 塞进 Prompt。
* **其他的 97 条无关反思（比如怎么钓鱼、怎么做饭）虽然存下来了，但不会占用当前的 Prompt 空间。**



**这就完美解决了“纸条太多”的问题：** 脑容量（Prompt）有限，但笔记本（Database）是无限的。用的时候再翻书。

### 经典案例：Voyager (Minecraft Agent)

你刚才提到的“不更改权重”，Voyager 就是最典型的例子。它是怎么做到玩几十个小时 Minecraft 越来越强的？

它有一个 **Skill Library（技能库）**：

1. 它学会了“砍树”，就写一段 `mine_wood()` 的代码，存到数据库里。
2. 它学会了“做桌子”，就写一段 `craft_table()` 存进去。
3. 下次它想“造房子”时，它不需要把几千个技能都塞进 Prompt，它只需要从库里检索出 `mine_wood` 和 `craft_table` 这两个相关技能放在 Context 里就够了。

**这就实现了：权重完全没改，但能力永久保存。**

### 第四招（终极技）：真的去改权重（Fine-tuning）

如果你的“纸条”积累了成千上万张，而且都是非常高质量的经验，这时候再用数据库查可能都慢了。

到了这个阶段，大公司（比如 OpenAI 或垂直领域的公司）会做一件事：
**Distillation（蒸馏）。**

* 把这些积累下来的“错误-修正-正确代码”作为**训练数据**。
* 开启 SFT（监督微调），把这些经验**真正地烧录进模型的权重里**。
* **结果：** 不需要 Prompt 里的纸条，模型也能天生做对。这就是从“In-Context Learning”进化到了“Fine-tuning”。

### 总结
“纸条太多”在工程上是通过**外部存储（External Memory）**解决的。

**Agent 的大脑结构其实是这样的：**

* **CPU:** LLM (处理当前任务)
* **RAM:** Context Window (放几张最急用的纸条)
* **Hard Disk:** Vector Database (存放成千上万条历史反思)
